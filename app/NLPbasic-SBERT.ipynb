{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cd2c44-cb02-46b0-87b7-69762a4bfce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@Author: Magnus Graham\\n1/3/2025\\n\\nThis notebook matches free text to a set of predefined symptoms.\\nIt uses spaCy to preprocess text for, and uses BioBERT and SBERT\\nto map their meaning to the closest possible match in the symptoms list.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@Author: Magnus Graham\n",
    "1/3/2025\n",
    "\n",
    "This notebook matches free text to a set of predefined symptoms.\n",
    "It uses spaCy to preprocess text for, and uses BioBERT and SBERT\n",
    "to map their meaning to the closest possible match in the symptoms list.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873b194-1d80-4a4c-8952-eea6343d3cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.11/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install torch\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm  # if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9ea6d-3bbe-42bc-8b6a-043a17ee6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load spaCy model for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load BioBERT model and tokenizer from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c58937-0e84-4b08-8924-5f33fc8963e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Lowercase and remove unnecessary punctuation\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_input(user_input):\n",
    "    # Normalize input text\n",
    "    normalized_input = normalize_text(user_input)\n",
    "    \n",
    "    # Split by commas or periods\n",
    "    clauses = [clause.strip() for clause in normalized_input.split(\",\") if clause.strip()]\n",
    "    print(\"processing\")\n",
    "    return clauses\n",
    "\n",
    "def process_clauses(clauses, lemmatize=True):\n",
    "    print(\"Processing clauses\")\n",
    "    processed_clauses = []  # Initialize the list to store processed clauses\n",
    "    for clause in clauses:\n",
    "        doc = nlp(clause)\n",
    "        \n",
    "        if lemmatize:\n",
    "            # Lemmatize each token in the clause\n",
    "            processed_clause = \" \".join([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])\n",
    "        else:\n",
    "            # Tokenize and exclude stop words and non-alphabetical tokens (without lemmatizing)\n",
    "            processed_clause = \" \".join([token.text for token in doc if token.is_alpha and not token.is_stop])\n",
    "        \n",
    "        processed_clauses.append(processed_clause)  # Append the processed clause to the list\n",
    "    return processed_clauses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e526360-88cc-4b34-8db7-b7b5994cf47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_bert_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    Generate SBERT embeddings for a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of input sentences.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Embedding tensor of shape (batch_size, hidden_size).\n",
    "    \"\"\"\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]  # Ensure input is a list\n",
    "\n",
    "    # Directly encode the sentences\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)  # Output shape: (batch_size, hidden_size)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168e6c4-7f5d-45ea-bf8a-334e2bf06a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to compute pairwise cosine similarity\n",
    "def cosine_similarity_matrix(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity matrix between two sets of embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings1 (torch.Tensor): Tensor of shape (n1, hidden_size).\n",
    "        embeddings2 (torch.Tensor): Tensor of shape (n2, hidden_size).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Similarity matrix of shape (n1, n2).\n",
    "    \"\"\"\n",
    "    return torch.mm(F.normalize(embeddings1, p=2, dim=1), F.normalize(embeddings2, p=2, dim=1).T)\n",
    "\n",
    "# Function to find the most similar predefined symptoms for each clause\n",
    "def compare_clauses_to_symptoms(clauses, predefined_symptoms, top_n=10):\n",
    "    \"\"\"\n",
    "    Matches clauses to predefined symptoms using cosine similarity of SBERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        clauses (list of str): User input clauses to compare.\n",
    "        predefined_symptoms (list of str): List of predefined symptoms.\n",
    "        top_n (int): Number of most similar symptoms to return for each clause.\n",
    "    \n",
    "    Returns:\n",
    "        None: Prints the top N matches for each clause.\n",
    "    \"\"\"\n",
    "    print(\"Comparing...\")\n",
    "    \n",
    "    # Get SBERT embeddings for clauses and symptoms\n",
    "    clause_embeddings = get_bio_bert_embeddings(clauses)\n",
    "    symptom_embeddings = get_bio_bert_embeddings(predefined_symptoms)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity_matrix(clause_embeddings, symptom_embeddings)\n",
    "    \n",
    "    # Process each clause and its similarities to symptoms\n",
    "    for i, clause in enumerate(clauses):\n",
    "        # Get similarity scores for the current clause\n",
    "        similarities = similarity_matrix[i].tolist()\n",
    "        \n",
    "        # Combine symptoms with their respective similarity scores\n",
    "        similarity_scores = [(predefined_symptoms[j], score) for j, score in enumerate(similarities)]\n",
    "        \n",
    "        # Extract the top N most similar symptoms\n",
    "        top_similar_symptoms = heapq.nlargest(top_n, similarity_scores, key=lambda x: x[1])\n",
    "        \n",
    "        # Display the top N matched symptoms for the clause\n",
    "        print(f\"Clause: '{clause}'\")\n",
    "        for symptom, similarity in top_similar_symptoms:\n",
    "            print(f\"  - Symptom: '{symptom}' - Similarity: {similarity:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7e029-32ea-43a3-8ed5-2460373a0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = \"/Users/magnusgraham/Downloads/disease-symptom/DiseaseAndSymptoms.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#create a set to store all unique symptom values\n",
    "predefined_symptoms = set()\n",
    "disease_names = set()\n",
    "    \n",
    "for col in df.columns[1:19]:  # Adjust indices if needed\n",
    "    for value in df[col].unique():\n",
    "        predefined_symptoms.add(str(value).replace(\"_\", \" \"))\n",
    "predefined_symptoms = list(predefined_symptoms)\n",
    "\n",
    "#add user inputted symptoms\n",
    "user_input = \"\"\"chills, dehydration, fatigue, fever, flushing, loss of appetite, body ache, or sweating\n",
    "Nasal: congestion, runny nose, or sneezing\n",
    "Also common: chest pressure, head congestion, headache, nausea, shortness of breath, sore throat, or swollen lymph nodes\"\"\"\n",
    "# Step 1: Preprocess the input\n",
    "clauses = preprocess_input(user_input)\n",
    "\n",
    "\n",
    "# Step 2: Tokenize and Lemmatize the clauses\n",
    "processed_clauses = process_clauses(clauses)\n",
    "processed_symptoms = process_clauses(predefined_symptoms)\n",
    "\n",
    "# Step 3: Compare each clause to predefined symptoms using BioBERT\n",
    "compare_clauses_to_symptoms(processed_clauses, processed_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494f071-ca9a-423a-b44e-1ab14c54b8d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
